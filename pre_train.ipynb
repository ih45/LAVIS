{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_path = \"/mnt/wjr/HTS-Audio-Transformer/ckpt/HTSAT_AudioSet_Saved_1.ckpt\"\n",
    "state_dict = torch.load(ckpt_path)['state_dict']\n",
    "new_state_dict = {}\n",
    "for key in state_dict.keys():\n",
    "    new_key = key[10:]\n",
    "    new_state_dict[new_key] = state_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['spectrogram_extractor.stft.conv_real.weight', 'spectrogram_extractor.stft.conv_imag.weight', 'logmel_extractor.melW', 'bn0.weight', 'bn0.bias', 'bn0.running_mean', 'bn0.running_var', 'bn0.num_batches_tracked', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed.norm.weight', 'patch_embed.norm.bias', 'layers.0.blocks.0.norm1.weight', 'layers.0.blocks.0.norm1.bias', 'layers.0.blocks.0.attn.relative_position_bias_table', 'layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.attn.qkv.weight', 'layers.0.blocks.0.attn.qkv.bias', 'layers.0.blocks.0.attn.proj.weight', 'layers.0.blocks.0.attn.proj.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.0.mlp.fc1.weight', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.weight', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.norm1.weight', 'layers.0.blocks.1.norm1.bias', 'layers.0.blocks.1.attn.relative_position_bias_table', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.attn.qkv.weight', 'layers.0.blocks.1.attn.qkv.bias', 'layers.0.blocks.1.attn.proj.weight', 'layers.0.blocks.1.attn.proj.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.0.blocks.1.mlp.fc1.weight', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.weight', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.downsample.reduction.weight', 'layers.0.downsample.norm.weight', 'layers.0.downsample.norm.bias', 'layers.1.blocks.0.norm1.weight', 'layers.1.blocks.0.norm1.bias', 'layers.1.blocks.0.attn.relative_position_bias_table', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.qkv.weight', 'layers.1.blocks.0.attn.qkv.bias', 'layers.1.blocks.0.attn.proj.weight', 'layers.1.blocks.0.attn.proj.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.0.mlp.fc1.weight', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.weight', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.norm1.weight', 'layers.1.blocks.1.norm1.bias', 'layers.1.blocks.1.attn.relative_position_bias_table', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.attn.qkv.weight', 'layers.1.blocks.1.attn.qkv.bias', 'layers.1.blocks.1.attn.proj.weight', 'layers.1.blocks.1.attn.proj.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.1.blocks.1.mlp.fc1.weight', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.weight', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.downsample.reduction.weight', 'layers.1.downsample.norm.weight', 'layers.1.downsample.norm.bias', 'layers.2.blocks.0.norm1.weight', 'layers.2.blocks.0.norm1.bias', 'layers.2.blocks.0.attn.relative_position_bias_table', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.qkv.weight', 'layers.2.blocks.0.attn.qkv.bias', 'layers.2.blocks.0.attn.proj.weight', 'layers.2.blocks.0.attn.proj.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.0.mlp.fc1.weight', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.weight', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.norm1.weight', 'layers.2.blocks.1.norm1.bias', 'layers.2.blocks.1.attn.relative_position_bias_table', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.attn.qkv.weight', 'layers.2.blocks.1.attn.qkv.bias', 'layers.2.blocks.1.attn.proj.weight', 'layers.2.blocks.1.attn.proj.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.1.mlp.fc1.weight', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.weight', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.2.norm1.weight', 'layers.2.blocks.2.norm1.bias', 'layers.2.blocks.2.attn.relative_position_bias_table', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.attn.qkv.weight', 'layers.2.blocks.2.attn.qkv.bias', 'layers.2.blocks.2.attn.proj.weight', 'layers.2.blocks.2.attn.proj.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.weight', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.weight', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.norm1.weight', 'layers.2.blocks.3.norm1.bias', 'layers.2.blocks.3.attn.relative_position_bias_table', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.attn.qkv.weight', 'layers.2.blocks.3.attn.qkv.bias', 'layers.2.blocks.3.attn.proj.weight', 'layers.2.blocks.3.attn.proj.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.weight', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.weight', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm1.weight', 'layers.2.blocks.4.norm1.bias', 'layers.2.blocks.4.attn.relative_position_bias_table', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.attn.qkv.weight', 'layers.2.blocks.4.attn.qkv.bias', 'layers.2.blocks.4.attn.proj.weight', 'layers.2.blocks.4.attn.proj.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.weight', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.weight', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.norm1.weight', 'layers.2.blocks.5.norm1.bias', 'layers.2.blocks.5.attn.relative_position_bias_table', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.attn.qkv.weight', 'layers.2.blocks.5.attn.qkv.bias', 'layers.2.blocks.5.attn.proj.weight', 'layers.2.blocks.5.attn.proj.bias', 'layers.2.blocks.5.norm2.weight', 'layers.2.blocks.5.norm2.bias', 'layers.2.blocks.5.mlp.fc1.weight', 'layers.2.blocks.5.mlp.fc1.bias', 'layers.2.blocks.5.mlp.fc2.weight', 'layers.2.blocks.5.mlp.fc2.bias', 'layers.2.downsample.reduction.weight', 'layers.2.downsample.norm.weight', 'layers.2.downsample.norm.bias', 'layers.3.blocks.0.norm1.weight', 'layers.3.blocks.0.norm1.bias', 'layers.3.blocks.0.attn.relative_position_bias_table', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.attn.qkv.weight', 'layers.3.blocks.0.attn.qkv.bias', 'layers.3.blocks.0.attn.proj.weight', 'layers.3.blocks.0.attn.proj.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.0.mlp.fc1.weight', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.weight', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.1.norm1.weight', 'layers.3.blocks.1.norm1.bias', 'layers.3.blocks.1.attn.relative_position_bias_table', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.attn.qkv.weight', 'layers.3.blocks.1.attn.qkv.bias', 'layers.3.blocks.1.attn.proj.weight', 'layers.3.blocks.1.attn.proj.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'layers.3.blocks.1.mlp.fc1.weight', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.weight', 'layers.3.blocks.1.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'tscam_conv.weight', 'tscam_conv.bias', 'head.weight', 'head.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(new_state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = \"/mnt/wjr/LAVIS/lavis/models/htsat_models/ckpt/HTSAT_AudioSet_Saved_1.ckpt\"\n",
    "torch.save(new_state_dict, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model\n",
    "model = load_model(\"blip2\", \"pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from lavis.processors.blip_processors import BlipImageTrainProcessor\n",
    "image_path = \"/mnt1/wjr/coco/test2014/COCO_test2014_000000000001.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "vis_processor = BlipImageTrainProcessor(image_size=224)\n",
    "\n",
    "image_input = vis_processor(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(image_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 1408])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "image_embeds = model.ln_vision(model.visual_encoder(image_input))\n",
    "image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n",
    "\n",
    "query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "\n",
    "query_output = model.Qformer.bert(\n",
    "        query_embeds=query_tokens,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_atts,\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "print(image_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adgroup/anaconda3/envs/dcase2022/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/adgroup/anaconda3/envs/dcase2022/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484683044/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lavis.models.blap_models.blap_qformer import BlapQformer\n",
    "model = BlapQformer().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.processors.blap_processors import BlapAudioProcessor\n",
    "audio_processor = BlapAudioProcessor(sample_rate=32000, max_sec=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "audio_feats = torch.load(\"/mnt/wjr/LAVIS/test_dir/audio_features.pth\")\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "print(torch.any(torch.isnan(audio_feats)))\n",
    "#print(audio_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "12\n",
      "13\n",
      "21\n",
      "26\n",
      "28\n",
      "35\n",
      "47\n",
      "50\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"/mnt/wjr/LAVIS/test_dir/logmel.pth\")\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "# print(torch.any(torch.isnan(data)))\n",
    "for i in range(100):\n",
    "    if torch.any(torch.isnan(data[i,:,:])).item():\n",
    "        print(i)\n",
    "# print(data[10,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adgroup/anaconda3/envs/dcase2022/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/adgroup/anaconda3/envs/dcase2022/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484683044/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HTSAT_Swin_Transformer(\n",
       "  (spectrogram_extractor): Spectrogram(\n",
       "    (stft): STFT(\n",
       "      (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "      (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (logmel_extractor): LogmelFilterBank()\n",
       "  (spec_augmenter): SpecAugmentation(\n",
       "    (time_dropper): DropStripes()\n",
       "    (freq_dropper): DropStripes()\n",
       "  )\n",
       "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(64, 64), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(8, 8), num_heads=4\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(8, 8), num_heads=4\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(64, 64), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(32, 32), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(8, 8), num_heads=8\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(8, 8), num_heads=8\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(32, 32), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(16, 16), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(16, 16), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(8, 8), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(8, 8), num_heads=32\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(8, 8), num_heads=32\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "  (tscam_conv): Conv2d(768, 527, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
       "  (head): Linear(in_features=527, out_features=527, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from lavis.models.htsat_models.htsat import HTSAT_Swin_Transformer\n",
    "import lavis.models.htsat_models.config as htsat_config\n",
    "file_path = '/mnt/wjr/LAVIS/lavis/models/htsat_models/ckpt/HTSAT_AudioSet_Saved_1.ckpt'\n",
    "state_dict = torch.load(file_path)\n",
    "audio_encoder = HTSAT_Swin_Transformer(\n",
    "            spec_size=htsat_config.htsat_spec_size,\n",
    "            patch_size=htsat_config.htsat_patch_size,\n",
    "            in_chans=1,\n",
    "            num_classes=htsat_config.classes_num,\n",
    "            window_size=htsat_config.htsat_window_size,\n",
    "            config=htsat_config,\n",
    "            depths=htsat_config.htsat_depth,\n",
    "            embed_dim=htsat_config.htsat_dim,\n",
    "            patch_stride=htsat_config.htsat_stride,\n",
    "            num_heads=htsat_config.htsat_num_head)\n",
    "audio_encoder.load_state_dict(state_dict)\n",
    "audio_encoder = audio_encoder.eval()\n",
    "audio_encoder.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from lavis.processors.blap_processors import BlapAudioProcessor\n",
    "def gen_waveform(dir, num_sample, audio_processor):\n",
    "    files = os.listdir(dir)\n",
    "    assert len(files) >= num_sample\n",
    "    samples = files[0:100]\n",
    "\n",
    "    waveform = None\n",
    "    for i, sample in enumerate(samples):\n",
    "        path = dir + \"/\" + sample\n",
    "        y = audio_processor(path)\n",
    "\n",
    "        if waveform is None:\n",
    "            waveform = y\n",
    "        else:\n",
    "            waveform = np.vstack([waveform, y])\n",
    "\n",
    "    return waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 960000])\n"
     ]
    }
   ],
   "source": [
    "from lavis.processors.blap_processors import BlapAudioProcessor\n",
    "audio_processor = BlapAudioProcessor(sample_rate=32000, max_sec=30)\n",
    "waveform = gen_waveform(\"/mnt1/datasets/Clotho/clotho_audio_development/development\", 100, audio_processor)\n",
    "waveform = torch.from_numpy(waveform).to(\"cuda\")\n",
    "print(waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = audio_encoder.spectrogram_extractor(waveform)\n",
    "x = audio_encoder.logmel_extractor(x)\n",
    "print(torch.any(torch.isnan(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"/mnt/wjr/LAVIS/test_dir/spec.pth\")\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "data.to(\"cuda\")\n",
    "#spec = audio_encoder.spectrogram_extractor(data)\n",
    "#print(spec.shape)\n",
    "logmel = audio_encoder.logmel_extractor(data)\n",
    "print(torch.any(torch.isnan(logmel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "new_logmel = torch.load(\"/mnt/wjr/LAVIS/test_dir/new_logmel.pth\")\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "print(torch.any(torch.isnan(new_logmel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_path = \"/mnt/wjr/LAVIS/lavis/models/htsat_models/ckpt/HTSAT_AudioSet_Saved_1.ckpt\"\n",
    "state_dict = torch.load(ckpt_path)\n",
    "new_state_dict = {}\n",
    "keys = ['spectrogram_extractor.stft.conv_real.weight', 'spectrogram_extractor.stft.conv_imag.weight', 'logmel_extractor.melW']\n",
    "for k in keys:\n",
    "    new_state_dict[k] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = \"/mnt/wjr/LAVIS/lavis/models/htsat_models/ckpt/gen_logmel.ckpt\"\n",
    "torch.save(new_state_dict, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['query_tokens', 'temp', 'audio_encoder.bn0.running_mean', 'audio_encoder.bn0.running_var', 'audio_encoder.bn0.num_batches_tracked', 'audio_encoder.layers.0.blocks.0.attn.relative_position_index', 'audio_encoder.layers.0.blocks.1.attn_mask', 'audio_encoder.layers.0.blocks.1.attn.relative_position_index', 'audio_encoder.layers.1.blocks.0.attn.relative_position_index', 'audio_encoder.layers.1.blocks.1.attn_mask', 'audio_encoder.layers.1.blocks.1.attn.relative_position_index', 'audio_encoder.layers.2.blocks.0.attn.relative_position_index', 'audio_encoder.layers.2.blocks.1.attn_mask', 'audio_encoder.layers.2.blocks.1.attn.relative_position_index', 'audio_encoder.layers.2.blocks.2.attn.relative_position_index', 'audio_encoder.layers.2.blocks.3.attn_mask', 'audio_encoder.layers.2.blocks.3.attn.relative_position_index', 'audio_encoder.layers.2.blocks.4.attn.relative_position_index', 'audio_encoder.layers.2.blocks.5.attn_mask', 'audio_encoder.layers.2.blocks.5.attn.relative_position_index', 'audio_encoder.layers.3.blocks.0.attn.relative_position_index', 'audio_encoder.layers.3.blocks.1.attn.relative_position_index', 'ln_audio.weight', 'ln_audio.bias', 'Qformer.bert.embeddings.position_ids', 'Qformer.bert.embeddings.word_embeddings.weight', 'Qformer.bert.embeddings.position_embeddings.weight', 'Qformer.bert.embeddings.LayerNorm.weight', 'Qformer.bert.embeddings.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.attention.self.query.weight', 'Qformer.bert.encoder.layer.0.attention.self.query.bias', 'Qformer.bert.encoder.layer.0.attention.self.key.weight', 'Qformer.bert.encoder.layer.0.attention.self.key.bias', 'Qformer.bert.encoder.layer.0.attention.self.value.weight', 'Qformer.bert.encoder.layer.0.attention.self.value.bias', 'Qformer.bert.encoder.layer.0.attention.output.dense.weight', 'Qformer.bert.encoder.layer.0.attention.output.dense.bias', 'Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.0.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.0.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.0.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.0.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.0.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.0.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.0.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.intermediate.dense.weight', 'Qformer.bert.encoder.layer.0.intermediate.dense.bias', 'Qformer.bert.encoder.layer.0.output.dense.weight', 'Qformer.bert.encoder.layer.0.output.dense.bias', 'Qformer.bert.encoder.layer.0.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.0.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.0.output_query.dense.weight', 'Qformer.bert.encoder.layer.0.output_query.dense.bias', 'Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.1.attention.self.query.weight', 'Qformer.bert.encoder.layer.1.attention.self.query.bias', 'Qformer.bert.encoder.layer.1.attention.self.key.weight', 'Qformer.bert.encoder.layer.1.attention.self.key.bias', 'Qformer.bert.encoder.layer.1.attention.self.value.weight', 'Qformer.bert.encoder.layer.1.attention.self.value.bias', 'Qformer.bert.encoder.layer.1.attention.output.dense.weight', 'Qformer.bert.encoder.layer.1.attention.output.dense.bias', 'Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.1.intermediate.dense.weight', 'Qformer.bert.encoder.layer.1.intermediate.dense.bias', 'Qformer.bert.encoder.layer.1.output.dense.weight', 'Qformer.bert.encoder.layer.1.output.dense.bias', 'Qformer.bert.encoder.layer.1.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.1.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.1.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.1.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.1.output_query.dense.weight', 'Qformer.bert.encoder.layer.1.output_query.dense.bias', 'Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.attention.self.query.weight', 'Qformer.bert.encoder.layer.2.attention.self.query.bias', 'Qformer.bert.encoder.layer.2.attention.self.key.weight', 'Qformer.bert.encoder.layer.2.attention.self.key.bias', 'Qformer.bert.encoder.layer.2.attention.self.value.weight', 'Qformer.bert.encoder.layer.2.attention.self.value.bias', 'Qformer.bert.encoder.layer.2.attention.output.dense.weight', 'Qformer.bert.encoder.layer.2.attention.output.dense.bias', 'Qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.2.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.2.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.2.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.2.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.2.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.2.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.2.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.intermediate.dense.weight', 'Qformer.bert.encoder.layer.2.intermediate.dense.bias', 'Qformer.bert.encoder.layer.2.output.dense.weight', 'Qformer.bert.encoder.layer.2.output.dense.bias', 'Qformer.bert.encoder.layer.2.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.2.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.2.output_query.dense.weight', 'Qformer.bert.encoder.layer.2.output_query.dense.bias', 'Qformer.bert.encoder.layer.2.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.3.attention.self.query.weight', 'Qformer.bert.encoder.layer.3.attention.self.query.bias', 'Qformer.bert.encoder.layer.3.attention.self.key.weight', 'Qformer.bert.encoder.layer.3.attention.self.key.bias', 'Qformer.bert.encoder.layer.3.attention.self.value.weight', 'Qformer.bert.encoder.layer.3.attention.self.value.bias', 'Qformer.bert.encoder.layer.3.attention.output.dense.weight', 'Qformer.bert.encoder.layer.3.attention.output.dense.bias', 'Qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.3.intermediate.dense.weight', 'Qformer.bert.encoder.layer.3.intermediate.dense.bias', 'Qformer.bert.encoder.layer.3.output.dense.weight', 'Qformer.bert.encoder.layer.3.output.dense.bias', 'Qformer.bert.encoder.layer.3.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.3.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.3.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.3.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.3.output_query.dense.weight', 'Qformer.bert.encoder.layer.3.output_query.dense.bias', 'Qformer.bert.encoder.layer.3.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.3.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.attention.self.query.weight', 'Qformer.bert.encoder.layer.4.attention.self.query.bias', 'Qformer.bert.encoder.layer.4.attention.self.key.weight', 'Qformer.bert.encoder.layer.4.attention.self.key.bias', 'Qformer.bert.encoder.layer.4.attention.self.value.weight', 'Qformer.bert.encoder.layer.4.attention.self.value.bias', 'Qformer.bert.encoder.layer.4.attention.output.dense.weight', 'Qformer.bert.encoder.layer.4.attention.output.dense.bias', 'Qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.4.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.4.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.4.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.4.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.4.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.4.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.4.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.intermediate.dense.weight', 'Qformer.bert.encoder.layer.4.intermediate.dense.bias', 'Qformer.bert.encoder.layer.4.output.dense.weight', 'Qformer.bert.encoder.layer.4.output.dense.bias', 'Qformer.bert.encoder.layer.4.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.4.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.4.output_query.dense.weight', 'Qformer.bert.encoder.layer.4.output_query.dense.bias', 'Qformer.bert.encoder.layer.4.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.5.attention.self.query.weight', 'Qformer.bert.encoder.layer.5.attention.self.query.bias', 'Qformer.bert.encoder.layer.5.attention.self.key.weight', 'Qformer.bert.encoder.layer.5.attention.self.key.bias', 'Qformer.bert.encoder.layer.5.attention.self.value.weight', 'Qformer.bert.encoder.layer.5.attention.self.value.bias', 'Qformer.bert.encoder.layer.5.attention.output.dense.weight', 'Qformer.bert.encoder.layer.5.attention.output.dense.bias', 'Qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.5.intermediate.dense.weight', 'Qformer.bert.encoder.layer.5.intermediate.dense.bias', 'Qformer.bert.encoder.layer.5.output.dense.weight', 'Qformer.bert.encoder.layer.5.output.dense.bias', 'Qformer.bert.encoder.layer.5.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.5.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.5.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.5.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.5.output_query.dense.weight', 'Qformer.bert.encoder.layer.5.output_query.dense.bias', 'Qformer.bert.encoder.layer.5.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.5.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.attention.self.query.weight', 'Qformer.bert.encoder.layer.6.attention.self.query.bias', 'Qformer.bert.encoder.layer.6.attention.self.key.weight', 'Qformer.bert.encoder.layer.6.attention.self.key.bias', 'Qformer.bert.encoder.layer.6.attention.self.value.weight', 'Qformer.bert.encoder.layer.6.attention.self.value.bias', 'Qformer.bert.encoder.layer.6.attention.output.dense.weight', 'Qformer.bert.encoder.layer.6.attention.output.dense.bias', 'Qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.6.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.6.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.6.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.6.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.6.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.6.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.6.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.intermediate.dense.weight', 'Qformer.bert.encoder.layer.6.intermediate.dense.bias', 'Qformer.bert.encoder.layer.6.output.dense.weight', 'Qformer.bert.encoder.layer.6.output.dense.bias', 'Qformer.bert.encoder.layer.6.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.6.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.6.output_query.dense.weight', 'Qformer.bert.encoder.layer.6.output_query.dense.bias', 'Qformer.bert.encoder.layer.6.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.7.attention.self.query.weight', 'Qformer.bert.encoder.layer.7.attention.self.query.bias', 'Qformer.bert.encoder.layer.7.attention.self.key.weight', 'Qformer.bert.encoder.layer.7.attention.self.key.bias', 'Qformer.bert.encoder.layer.7.attention.self.value.weight', 'Qformer.bert.encoder.layer.7.attention.self.value.bias', 'Qformer.bert.encoder.layer.7.attention.output.dense.weight', 'Qformer.bert.encoder.layer.7.attention.output.dense.bias', 'Qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.7.intermediate.dense.weight', 'Qformer.bert.encoder.layer.7.intermediate.dense.bias', 'Qformer.bert.encoder.layer.7.output.dense.weight', 'Qformer.bert.encoder.layer.7.output.dense.bias', 'Qformer.bert.encoder.layer.7.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.7.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.7.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.7.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.7.output_query.dense.weight', 'Qformer.bert.encoder.layer.7.output_query.dense.bias', 'Qformer.bert.encoder.layer.7.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.7.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.attention.self.query.weight', 'Qformer.bert.encoder.layer.8.attention.self.query.bias', 'Qformer.bert.encoder.layer.8.attention.self.key.weight', 'Qformer.bert.encoder.layer.8.attention.self.key.bias', 'Qformer.bert.encoder.layer.8.attention.self.value.weight', 'Qformer.bert.encoder.layer.8.attention.self.value.bias', 'Qformer.bert.encoder.layer.8.attention.output.dense.weight', 'Qformer.bert.encoder.layer.8.attention.output.dense.bias', 'Qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.8.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.8.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.8.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.8.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.8.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.8.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.8.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.intermediate.dense.weight', 'Qformer.bert.encoder.layer.8.intermediate.dense.bias', 'Qformer.bert.encoder.layer.8.output.dense.weight', 'Qformer.bert.encoder.layer.8.output.dense.bias', 'Qformer.bert.encoder.layer.8.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.8.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.8.output_query.dense.weight', 'Qformer.bert.encoder.layer.8.output_query.dense.bias', 'Qformer.bert.encoder.layer.8.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.9.attention.self.query.weight', 'Qformer.bert.encoder.layer.9.attention.self.query.bias', 'Qformer.bert.encoder.layer.9.attention.self.key.weight', 'Qformer.bert.encoder.layer.9.attention.self.key.bias', 'Qformer.bert.encoder.layer.9.attention.self.value.weight', 'Qformer.bert.encoder.layer.9.attention.self.value.bias', 'Qformer.bert.encoder.layer.9.attention.output.dense.weight', 'Qformer.bert.encoder.layer.9.attention.output.dense.bias', 'Qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.9.intermediate.dense.weight', 'Qformer.bert.encoder.layer.9.intermediate.dense.bias', 'Qformer.bert.encoder.layer.9.output.dense.weight', 'Qformer.bert.encoder.layer.9.output.dense.bias', 'Qformer.bert.encoder.layer.9.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.9.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.9.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.9.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.9.output_query.dense.weight', 'Qformer.bert.encoder.layer.9.output_query.dense.bias', 'Qformer.bert.encoder.layer.9.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.9.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.attention.self.query.weight', 'Qformer.bert.encoder.layer.10.attention.self.query.bias', 'Qformer.bert.encoder.layer.10.attention.self.key.weight', 'Qformer.bert.encoder.layer.10.attention.self.key.bias', 'Qformer.bert.encoder.layer.10.attention.self.value.weight', 'Qformer.bert.encoder.layer.10.attention.self.value.bias', 'Qformer.bert.encoder.layer.10.attention.output.dense.weight', 'Qformer.bert.encoder.layer.10.attention.output.dense.bias', 'Qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.10.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.10.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.10.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.10.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.10.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.10.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.10.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.intermediate.dense.weight', 'Qformer.bert.encoder.layer.10.intermediate.dense.bias', 'Qformer.bert.encoder.layer.10.output.dense.weight', 'Qformer.bert.encoder.layer.10.output.dense.bias', 'Qformer.bert.encoder.layer.10.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.10.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.10.output_query.dense.weight', 'Qformer.bert.encoder.layer.10.output_query.dense.bias', 'Qformer.bert.encoder.layer.10.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.11.attention.self.query.weight', 'Qformer.bert.encoder.layer.11.attention.self.query.bias', 'Qformer.bert.encoder.layer.11.attention.self.key.weight', 'Qformer.bert.encoder.layer.11.attention.self.key.bias', 'Qformer.bert.encoder.layer.11.attention.self.value.weight', 'Qformer.bert.encoder.layer.11.attention.self.value.bias', 'Qformer.bert.encoder.layer.11.attention.output.dense.weight', 'Qformer.bert.encoder.layer.11.attention.output.dense.bias', 'Qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.11.intermediate.dense.weight', 'Qformer.bert.encoder.layer.11.intermediate.dense.bias', 'Qformer.bert.encoder.layer.11.output.dense.weight', 'Qformer.bert.encoder.layer.11.output.dense.bias', 'Qformer.bert.encoder.layer.11.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.11.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.11.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.11.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.11.output_query.dense.weight', 'Qformer.bert.encoder.layer.11.output_query.dense.bias', 'Qformer.bert.encoder.layer.11.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.11.output_query.LayerNorm.bias', 'Qformer.cls.predictions.bias', 'Qformer.cls.predictions.transform.dense.weight', 'Qformer.cls.predictions.transform.dense.bias', 'Qformer.cls.predictions.transform.LayerNorm.weight', 'Qformer.cls.predictions.transform.LayerNorm.bias', 'Qformer.cls.predictions.decoder.weight', 'Qformer.cls.predictions.decoder.bias', 'audio_proj.weight', 'audio_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ckpt_path = \"/mnt/wjr/LAVIS/lavis/output/BLAP/Pretrain_stage1/20230720130/checkpoint_0.pth\"\n",
    "state_dict = torch.load(ckpt_path)[\"model\"]\n",
    "#new_state_dict = {}\n",
    "#for key in state_dict.keys():\n",
    "#    new_key = key[10:]\n",
    "#    new_state_dict[new_key] = state_dict[key]\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
